---
title: "Pollen_Mapper"
author: "Anna George, Williams Lab" 
affiliation: "University of Wisconsin - Madison"
date: "2/27/2019"
#bibliography: ?
#output:
  #html_document:
    #code_folding: {show?}
    #fig_caption: {yes/no}
    #keep_md: {yes/no}
    #self_contained: {yes/no}
    #theme: {readable?}
    #toc: {yes/no}
    #toc_float: {yes/no}
#dev: {svg?}
#highlight: {tango?}
#keywords: {chronology, geochronology, paleoecology, age-models,
  #Bacon, 210Pb, 14C, radiocarbon, biostratigraphy, Bayesian}
#csl: styles/elsevier-harvard.csl

---

##Introduction

Welcome to (what are you calling this lol), a repository (is this the right word?) that allows researchers, web developers and other users to extract species-specifc(can it?), spatio-temporal data from the Neotoma Paleoecology Database [@williams2017neotoma]. This code also establishes a channel between Neotoma and Carto, an online platform where Neotoma data can now be visualized and transformed through Carto's user interface.[https://carto.com/attribution/] (NOTE FOR ANNA-- how/what do we site here? Is this what's happening?). 

Building this R pathway was helpful to us in our attempts to model global forest turnover over the last 21,000 years, using on pollen records archived on Neotoma. Our models are a few of many that could be possible with code that links Neotoma's deep paleoecological data repository to Carto's intuitive and vibrant mapping technology.

Our code integrates previous methods established by @williams2017neotoma and @goring2018bulk-baconizing for extracting and interpolating Neotoma data in R with a quick solution for moving that data to Carto, developed in @dracodoc2017rCartoAPI. In this guide, we will provide step-by-step instructions for running all parts of the code.(@anna, what am I not citing here?)

#### RStudio ----I coppied this part straight from bulk baconizing. Is this standard? do I need to cite?

Many people working with R may choose to use [RStudio](http://rstudio.com).  If you are using RStudio, you can customize elements of this document (for example, removing this header information) and then use the *knit* button to compile the code into an HTML document, while generating the neccessary output files (see the [README file](https://github.com/neotomadb) in the GitHub repository).


### Providing Feedback

If, in any place instructions are not clear or more details are required please feel free to contact us by either [raising a GitHub Issue for this repository](Insert Link), or by emailing (?) directly at (?).


## Connecting to Neotoma

Is there a begining step that had to procede through terminal? I seem to remember doing this but can't find any doccumentation... I'll add it here if needed!

## Setting up your library

The Neotoma Package facilitates interactions between R and the Neotoma Database, and the dplyr Package allows you to filter the data you've extracted by factors like site and age.

Later on, the rCartoAPI Package will push the data you've downloaded to Carto. That process references tools contained in the httr and devtools packages.  

```{r library}
library(neotoma)
library(dplyr)

library(httr)
library(devtools)
library(rCartoAPI)
```

## Retrieving Neotoma data

With this set-up complete, you're ready to extract your first dataset! In this example, we'll compile a few pollen records from Australia. 

The taxa we'll be mapping are Nothofagus and Eucalyptus, but there are countless records available on Neotoma, and you can extract as many as you'd like by replicating the code below. However, it might be helpful to browse for any additional taxa on Neotoma, just to make sure that what you're looking for exists in its database.  Be aware that these values are case-sensitive. 

Then, select the geographical bounds of your querry and express them as coordinates. ttps://boundingbox.klokantech.com/ is a useful way to generate your own bounding regions. Otherwise, these coordinates worked well for us across the sites we surveyed:

 North American bounding box: loc = c(-130, 24, -34, 65),
 European bounding box: loc = c(-11, 35, 47, 72)
 Australian bounding box: loc = c(105, -51, 177, 10)
 
It's also possible to select data from a geopolitical region. That code is recorded in lines 115-117. Pick whichever of these methods work best for you. 

Third, set your boundary ages. We use a minimum of 21,000 years ago and set our maximum to 0. 

Combined, these parameters are recorded in the metadata for the taxon in a specified location.

The final step creates a list of all the taxon dataset numbers.

```{r data_query}

nothofagus_datasets <- get_dataset(taxonname = 'Nothofagus*',
                              loc = c(105, -51, 177, 10),
                              ageyoung = 0,
                              ageold = 21000)

eucalyptus_datasets <- get_dataset(taxonname = 'Eucalyptus*',
                                loc = c(105, -51, 177, 10), 
                                ageyoung = 0, 
                                ageold = 21000)

#can_datasets <- neotoma::get_dataset(datasettype = 'pollen', 
                                     #ageold = 15000, ageyoung = -100,
                                     #gpid = "Canada")

nothofagus_dataset_numbers <- as.numeric(names(nothofagus_datasets))
eucalyptus_dataset_numbers <- as.numeric(names(eucalyptus_datasets))

```

## Appending and downloading your lists

The first line in this dataset prepares the empty list you'll fill with your extracted data later. Nothing to add to this line!

The second line combines lists of different taxa dataset numbers for the records you've pulled. Add each taxon as a comma-separated argument to this vector.

The third line removes duplicate dataset numbers from your new list. Again, nothing to update here.

The last two lines download all your datasets as RDS files. You may want to change the name "data_downloads" to something more specific, especially if you plan on downloading multiple datasets. Set the file path to a pre-existing, local file. 

```{r download}
site_dataset_numbers = list()

site_dataset_numbers <- append(site_dataset_numbers, c(nothofagus_dataset_numbers,
                                                       eucalyptus_dataset_numbers))
all_dataset_numbers <- as.numeric(unique(site_dataset_numbers))

data_downloads <- get_download(all_dataset_numbers)

saveRDS(data_downloads, file = "~/Desktop/Neotoma.RData")
```

## Compliling Data

In this next section, you'll be working with data you've already downloaded from Neotoma. Now, you'll retrieve that data and prepare it for mapping. Note that data_downloads may be a generic name, if you've chosen to call your file something more specific.

```{r compile}
data_downloads <- readRDS("~/Desktop/Neotoma.RData")

comp_dl <- compile_downloads(data_downloads)
```

## Linear Interpolation

This chunk fetches total counts for each taxon observation at every site and linerally interpolates all the data in 500 year intervals. 


```{r linear interpolation}
tot_cnts <- rowSums(comp_dl[,11:ncol(comp_dl)], na.rm=TRUE)

interp_dl <- data.frame(comp_dl[,1:10],
                        time = - (round(comp_dl$age / 500, 0) * 500),
                        nothofagus = rowSums(comp_dl[, grep("Nothofagus*", colnames(comp_dl))], na.rm = TRUE) / tot_cnts,
                        eucalyptus = rowSums(comp_dl[, grep("Eucalyptus*", colnames(comp_dl))], na.rm = TRUE) / tot_cnts)
group_by(time, lat, long, site.name) %>%
summarize( nothofagus = mean ( nothofagus) * 100, 
           eucalyptus = mean ( eucalyptus) * 100)

```

## Clean your data

These next lines remove any observations from beyond the maxiumim age you recorded in your file.

```{r filter}
timefltr_output <- dplyr::filter(interp_dl, time >= -21000)
final_output <- na.omit(timefltr_output)
```

## Create CSV and upload it to Carto

This chunk takes the data frame you generated above, converts it to a CSV and then imediatly posts it to Carto. All you have to do is provide a file path and carto user information where prompted. If you are not sure if you are able to access Carto, we've included an optional test conection, which you may uncomment and run as you'd like. 

Double check your Carto data library to make sure yuour file has posted. Now you're ready to generate some rad visualizations!

```{r to_Carto}
write.csv(final_output, file = "~/Desktop/Neotoma.csv")
inputFile <- "~/Desktop/Neotoma.csv"


#This section posts the file you just created and saved locally to R

#our carto info
carto_acc = "username"
carto_api = "api key"

#optional:
#test_connection()

#Post the file!
local_import(inputFile)
```
